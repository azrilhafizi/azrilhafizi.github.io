[{"content":"As ML systems expand in size and capabilities, it\u0026rsquo;s crucial to prioritize safety research. Like any powerful technology, the responsible development and deployment of ML systems require a thorough understanding of potential risks and a dedication to mitigating them. In this blog post, I\u0026rsquo;ll share what I\u0026rsquo;ve learned from the Introduction to ML Safety course offered by the Center for AI Safety.\nThere are four main research areas to mitigate existential risks (X-Risks) from strong AI. These include robustness (withstanding hazards), monitoring (identifying hazards), control (reducing inherent ML system hazards), and systemic safety (reducing systemic hazards)\nSafety Engineering To effectively approach making ML systems safer, we need some sort of structure to guide our thinking. Safety engineering plays an important role in designing systems and technologies that prioritize user well-being and prevent potential harm. It includes a wide array of methodologies, models, and frameworks to understand and manage risks. Understanding safety engineering begins with defining crucial terms:\nFailure Mode: Possible ways in which a system might fail Hazard: A source of danger with the potential to cause harm Vulnerability: Factors or processes that increase susceptibility to the damaging effects of hazards Threat: A hazard with the intent to exploit vulnerabilities Exposure: The extent to which elements (e.g. people, property, systems, etc.) are subjected to hazards Ability to Cope: The ability to efficiently recover from the effects of hazards Risk Decomposition Given a set of hazardous events $H$ that we are concerned about, the risk of them can simply be defined as:\n$$ \\text{Risk} = \\sum_{h \\in H} P(h) \\text{ Impact}(h) $$\nNominally, we can define the risk as:\n$$ \\text{Risk} = \\text{Vulnerability} \\times \\text{Hazard Exposure} \\times \\text{Hazard} $$\nFor a more precise definition, we need to understand the risk in the context of our total exposure to them and how vulnerable our system is to them. We need to consider the probability of impact and the impact itself.\n$$ \\text{Risk} = \\sum_{h \\in H} P(h) \\times \\text{severity}(h) \\times P(\\text{contact} \\mid h) \\times \\text{vulnerability}(h) $$\nHere are examples of risks, broken down to their nominal components:\nVulnerability Hazard Exposure Hazard Injury from falling on wet floor Bodily brittleness Floor utilization Floor slipperiness Damage from flooding Low elevation Number of people Rainfall volume Flu-related health complications Old age, poor health Contact with flu carriers Flu prevalence and severity Now that we have decomposed the risks into their components, we can systematically consider how to reduce the risk. In the case of falling from a wet floor, we could:\nAddress the hazard itself by getting rid of the mop and bucket, and using autoscrubber instead Reduce exposure to hazards by putting up a warning sign Or, reduce our vulnerability to the risk by doing strength training to make our body less brittle Reduce Risks for ML System Using the risk decomposition, we can break down the risk posed by machine learning systems into parts. The hazard of a machine learning system stems from the alignment problem. Alignment researchers focus on how to reduce the probability and severity of inherent model hazards. The vulnerability of a machine learning system can be addressed by the robustness research field. Robustness considers how we can withstand hazards and thus, how we can reduce our vulnerability to them. Lastly, the hazard exposure of a machine learning system can be characterized by the monitoring research field, which is concerned with identifying hazards. When we can identify a potential hazard, the risk gets lower as we can take steps to limit our exposure to it.\nRobustness Adversarial Robustness Adversarial attack against a machine learning system is a technique to make a machine learning system misbehave. Often adversarial attacks revolve around the generation of adversarial examples, inputs designed to confuse or deceive the machine learning system. For example, attackers (or adversaries) could add imperceptible noise to an image classifier and make the classifier change its behavior. To humans, the image looked exactly the same, but to the model, the image had changed significantly.\nExpanding on this, a simple threat model is to assume adversary has an $l_p$ attack distortion budget, assuming a fixed $p \\text{ and } \\epsilon$:\n$$ ||x_\\text{adv} - x || \\leq \\epsilon $$\nThe adversary’s goal is to find a distortion that maximize the loss subject to its budget:\n$$ x_\\text{adv} = x + \\arg \\max \\mathcal{L} (x + \\delta, y ; \\theta) $$\nFast Gradient Sign Method Fast Gradient Sign Method (FGSM) is one of the earliest techniques in generating adversarial examples. It obeys an $l_\\infty$ attack budget $||x_\\text{adv} - x ||_\\infty = \\epsilon$. Let $\\mathcal{L}$ be some loss function (e.g., cross entropy) used to train our model with parameters $\\theta$. We then can generate adversarial examples as follows:\n$$ x_\\text{adv} = x + \\epsilon \\text{ sign}(\\nabla_x \\mathcal{L}(x, y;\\theta)) $$\nWe take the sign of the gradient indicates the direction in which the loss function increases the most rapidly. By taking the sign, the FGSM method effectively identifies the direction in which to perturb the input to maximize the loss. It also helps bound the gradient between -1 and 1. This bounding of the gradient makes $x_\\text{adv}$ naturally satisfy the $\\infty$-norm constraint.\nThe FGSM is fast and computationally efficient because it only requires a single forward and backward pass through the network to compute the gradient. However, it\u0026rsquo;s worth noting that FGSM is a relatively weak method for generating adversarial examples and can be defended against easily. Nonetheless, its simplicity makes it a useful baseline attack to expand upon or compare against.\nProjected Gradient Descent Unlike the single-step FGSM attack, Projected Gradient Descent (PGD) attack uses multiple gradient ascent steps and is thus far more powerful. It generate adversarial examples through iterative optimization process. PGD starts with initialization to introduce random perturbation to original input $x$:\n$$ x_\\text{adv} := x + n, ; \\text{where } n_i \\sim \\mathcal{U}[-\\varepsilon,\\varepsilon] $$\nThen it iterates to refines perturbation in direction that maximize the loss:\n$$ x_\\text{adv} = \\mathcal{P}(x_\\text{adv} + \\alpha \\text{sign}(\\nabla_{\\delta} \\mathcal{L}(x_\\text{adv} + \\delta, y; \\theta))) $$\nwhere $\\mathcal{P}(z) = \\texttt{clip}(z, x-\\varepsilon, x+\\varepsilon)$.\nIn practice, PGD generates very strong adversarial examples. There are many adversarial defenses which are broken by simply increasing the number of steps. On the flipside, the strength of PGD’s adversarial examples gives us a way to train models to be robust.\nAdversarial Training The best-known way to make models more robust to $l_p$ adversarial examples is by adversarial training. During adversarial training, we expose our model to adversarial examples and penalize our model if the model is deceived. In particular, an adversarial training loss might be as follows:\n$$ \\mathcal{L}(f, D) = \\mathbb{E}[\\text{CrossEntropy}(f(x),y) + \\lambda \\cdot \\text{CrossEntropy}(f(g_\\text{adv}(x)),y)] $$\nwhere $\\lambda$ is hyperparameter determining how much we emphasize the adversarial training. This often reduces accuracy, but increases robustness to the specific adversarial attack which our model is trained on.\nTransferability of Adversarial Examples An adversarial example crafted for one model are actually transferable to other models, including models which were trained on a disjoint training set (Szegedy et al. 2014; Goodfellow et al. 2015). Given models $\\mathcal{M}_1$ and $\\mathcal{M}_2$, $x_\\text{adv}$ designed for $\\mathcal{M}_1$ sometimes gives $\\mathcal{M}_2(x_\\text{adv})$ a high loss, even if the architectures are different. Even though the accuracy varies depending on attack method, model architecture, and datasets, transferability demonstrate that adversarial failure modes are somewhat shared across models.\nThis itself has lead to a new type of adversarial attack, which can be performed without access to the actual model. It involves training a surrogate model, attacking that surrogate model, and hoping the attack transfers to the unseen model (Papernot et al. 2016; Papernot et al. 2017).\nBlack Swan Robustness Black Swans are events that are outliers, lying outside typical expectations, and often carry extreme impact. Black Swans often discussed within the context of long-tailed distribution, where the probability of rare events occurring is higher than what would be expected under a normal or Gaussian distribution. Random variables $X_i$ from long-tailed distribution are often max-sum equivalent:\n$$ \\lim_{n \\to \\infty} \\frac{X_1 + \\cdots + X_n}{\\max{X_1, \\cdots, X_n}} = 1 $$\nThis means that the largest event matter more than all the other events combined.\nWe can measure vulnerability to long tail events by simulating them using stress-test datasets. The stress-test datasets are from different data generating process than the training data. The goal is to make model performance not degrade as sharply in the face of extreme stressors.\nBenchmarks Robustness to long tail events is usually measured by performance on robustness benchmarks. Importantly, these benchmarks should only be used for testing models, not training models. These robustness benchmarks are designed to test the robustness of ImageNet classifiers.\nImageNet-C ImageNet-C (Hendrycks and Dietterich 2019), which stands for “ImageNet Corruptions” is a benchmark created by modifying ImageNet. ImageNet-C applies fifteen common corruptions to ImageNet at five levels of severity each (for a total of 75 corruptions).\nThe most common metric used for ImageNet-C is mean corruption error or mCE. Simply put, it is an indicator of how much stronger against the image noise is compared to AlexNet. And it is the average of the five noise levels. The lower mCE means stronger model against the noise.\nImageNet-Sketch and ImageNet-R ImageNet-Sketch (Wang et al. 2019) and ImageNet-R(enditions) (Hendrycks, Basart, et al. 2021) are two similar datasets loosely based off of ImageNet. ImageNet-Sketch is a dataset which collected 50K new sketches of the various ImageNet-1k classes. ImageNet-R is a similar dataset which collected 30K renditions of various styles of the ImageNet-1k classes. Neither ImageNet-Sketch nor ImageNet-R is a subset of ImageNet. Instead, the images are curated from new images scraped off of the internet.\nThese datasets test whether a model can generalize from a dataset composed of largely real-world images to datasets consisting of sketches or other artistic renditions.\nImageNet-A ImageNet-A (Hendrycks, Zhao, et al. 2021) is a collection of images belonging to ImageNet classes, which have been adversarially filtered to be difficult for models to classify. Specifically, a large set of images are tested on two different ResNets, and only images which the ResNets both perform poorly on are included. In total, the dataset contains images spanning 200 ImageNet-1k classes. Similar to ImageNet-Sketch and ImageNet-R, ImageNet-A is also not a subset of ImageNet, but is curated from a separately collected set of images.\nImproving Long Tail Robustness Improving the robustness of ImageNet classifiers largely breaks down into three directions: data augmentation, architectural choices, and pretraining techniques.\nData Augmentation Data augmentation is the process of applying randomized transformations (e.g., horizontal flip, 30 degrees rotation, discoloration) on input examples to create additional valid data. Data augmentation improves robustness by increasing the diversity of a dataset to include perturbed or altered images.\nDirections in data augmentation include directly proposing new augmentations (Zhang et al. 2018; Geirhos et al. 2019; Hendrycks, Basart, et al. 2021; Hendrycks, Zou, et al. 2021), developing algorithms which find the best augmentations for any given dataset (Cubuk, Zoph, Mane, et al. 2019; Cubuk, Zoph, Shlens, et al. 2019), or composing multiple augmentations to further increase augmentation strength (Hendrycks et al. 2020; Hendrycks, Zou, et al. 2021).\nModel Architecture Improving model architecture also leads to gains in performance. Currently, there exist two main architecture types in computer vision: convolutional neural networks and transformers. Transformers are a network architecture based heavily on “attention,” a technique for quickly routing information through a neural network (Vaswani et al. 2017). With the introduction of the vision transformer (ViT), researchers quickly demonstrated that vision transformers are more robust compared to ResNets (Bhojanapalli et al. 2021; Morrison et al. 2021; Paul and Chen 2021). ViT’s robustness has been attributed to many of its architectural choices, but it’s generally agreed that its attention mechanism plays a big role in its robustness. For instance, if we limit a transformer’s attention to a local context, similar to the convolutional filters in a ConvNet, robustness decreases (Mao et al. 2021).\nFor convolutional neural network, researchers at Facebook released the ConvNeXt architecture (Liu et al. 2022) which started at a ResNet50 base and systematically improved the architecture until it was on par with the best vision transformer systems. Interestingly, although they optimize for ImageNet validation performance, they get very strong robustness results as a side effect.\nPretraining Techniques Facebook\u0026rsquo;s paper titled \u0026ldquo;Scalable Vision Learners: Masked Autoencoders\u0026rdquo; introduces a novel pretraining technique involving masking out large portions of the image and trying to reconstruct them. Masked Autoencoders (MAE) segment images into patches and randomly mask a significant portion (around 80%) of them. In practice, aggressive masking is required, or else a model will only learn to interpolate between patches rather than learn global representations of the image. Furthermore, MAE utilized vision transformers rather than convolutional networks. Vision transformers are designed to for global information aggregation and can deal with the aggressive masking better.\nFor scalability, MAE employs a two-step autoencoding approach. Initially, it uses a deep encoder to process only the non-masked patches. By only processing the patches which provide information, the encoder only needs to focus on 20% of the image. This, in turns, enables the encoder to scale to deeper architectures. Then, it uses a shallow decoder to reconstruct the whole image. This time, the decoder inputs both the encoded vectors of the non-masked patches as well as vectors for the masked patches. However, since the decoder is much more shallow than the encoder, processing the full image ends up not being a large computational burden as well.\nMonitoring Anomaly Detection Anomaly detection or out-of-distribution(OOD) detection research focus on detecting anomalies, novel threat and long-tailed event. When a machine learning system encounter an anomaly, it can trigger a conservative feedback policy.\nMetrics In anomaly detection, accuracy score are rarely used as it is not informative enough when there is imbalance between anomalies and usual examples. Assume we have 1 anomaly and 99 usual examples. If the model always predict the examples as “usual”, the accuracy is as high as 99%:\n$$ \\text{accuracy} = \\frac{\\text{TP+TN}}{\\text{TP+TN+FP+FN}} = \\frac{0+99}{0+99+0+1} = 99% $$\nIn other words, total accuracy is a bad metric for rare events. Instead, anomaly detection traditionally uses other metrics such as FPR 95 and AUROC.\nTo understand these metrics, we first must understand the concept of true positive rate and false positive rate. Consider a test on different images to detect anomalies. A positive means an image has been flagged as anomaly, whereas a negative means an image has been flagged as usual example. True positive rate is the probability that an usual examples image will be positive (what fraction of all anomalies are flagged). False positive rate is the proportion of positives which should be negatives (what fraction of all usual examples are flagged).\n$$ \\text{True positive rate} = \\frac{\\text{True positive}}{\\text{True positive} + \\text{False negative}} $$\n$$ \\text{False positive rate} = \\frac{\\text{False positive}}{\\text{True negative} + \\text{False positive}} $$\nFPR 95 FPR 95 shows false positive rate at 95% true positive rate. We start with selecting a threshold $t$ such that we detect 95% of the anomalies. Then, from the examples flagged as anomaly, we measure the proportion which are actually usual examples. Note that the 95% threshold is somewhat arbitrary. Other papers have used FPR 90 and, in principle, any FPR N could exist. However, FPR 95 is the current community standard.\nAUROC The issue with FPR 95 is that the 95 threshold is somewhat arbitrary and doesn’t reflect the model’s anomaly detection abilities over all other thresholds. Another metric, AUROC, fixes this by being threshold-invariant. Specifically, the receiver operating curve (ROC) is the curve which graphs the true positive rate against the false positive rate. The area under the ROC (AUROC) measures the total area under the curve. AUROC can also be interpreted as the probability that a usual example has a higher confidence score than an anomaly. An AUROC of 0.5 is random, and an AUROC of 1.0 corresponds to a perfect classifier. Some advantages of using AUROC is it works even without calibration. Its also does not depend on ratio of positive to negative, highly useful when anomalies are far less frequent than usual example.\nBenchmark Anomaly detection does not need advance curation of datasets. Simply take two datasets, label one as in-distribution and one as out-of-distribution the benchmark is created. Another common way to generate anomaly detection benchmarks is to use one-class classification. We train a model to recognize only a single class within a dataset. All other examples from all other classes are treated as out-of-distribution.\nIn image classification, anomaly detection methods are validated on a variety of small datasets. Common datasets include Cifar10 (Krizhevsky 2009), Cifar100 (Krizhevsky 2009), Street View House Numbers (SVHN) (Netzer et al. 2011), TinyImageNet (Le and Yang 2015), Large-scale Scene Understanding (LSUN) (Yu et al. 2016). Usually, either Cifar10 or SVHN is in-distribution and the remaining are out-of-distribution. Cifar10 is also commonly used for one-class classification. In general, the more similar the two datasets are, the more difficult anomaly detection is.\nAt a larger scale, researchers often use ImageNet-1K (Deng et al. 2009), ImageNet-22K (a superset of ImageNet with 22 thousand classes) (Deng et al. 2009), and LSUN (large scale scene understanding) (Yu et al. 2016). Large-scale one-class classification is often done with either ImageNet-1K or ImageNet-22K.\nCalibration Formally, calibration is defined as follows. Let $\\hat{y}$ be the class prediction for $x$ and let $\\hat{p}(\\hat{y} \\mid x)$ be its associated confidence. A classifier if perfectly calibrated if $\\mathbb{P}(y=\\hat{y} \\mid \\hat{p}(\\hat{y} \\mid x)) = \\hat{p}(\\hat{y} \\mid x)$. In other word, calibration involve ensuring that a model’s output probability distribution matches the actual probability of a correct prediction. If a model is perfectly calibrated, when it gives a 90% confidence to a certain prediction, the prediction will be correct 90% of the time.\nCalibrated models can better convey the limit of their competency by expressing their uncertainty. We can then know when to override the model. Model confidences also are more interpretable the more they are calibrated.\nMetrics To evaluate calibration, we can use four different metrics: expected calibration error, maximal calibration error, negative log likelihood, and brier score. Expected calibration error is by far the most common, although the other metrics are also sometimes used.\nExpected Calibration Error Expected calibration error (ECE) is the primary metric for testing calibration (Naeini, Cooper, and Hauskrecht 2015). ECE measures the average discrepancy between the predicted probabilities and the actual outcomes over different confidence intervals. To calculate ECE, we first divide the confidence interval into equally spaced bins. For instance, we might let the bins be [0, 0.1], [0.1, 0.2], … [0.9, 1]. Then we compute the absolute difference between the average predicted probability and the actual probability for each bin. Taking the weighted average of these differences gives the ECE.\nFormally, say we have $n$ examples partitioned up into $M$ bins $B_1, B_2, \\cdots, B_M$. Let $\\text{acc}(B_M)$ be the average accuracy of examples in the bin and let $\\text{conf}(B_M)$ be the average confidence of examples in the bin. Then ECE is defined as:\n$$ \\text{ECE} = \\sum_{m=1}^M\\frac{|B_m|}{n}\\left|\\text{acc}(B_m) - \\text{conf}(B_m)\\right| $$\nECE ranges between 0 and 1, with lower scores being better. A low ECE indicates good calibration, meaning that the predicted probabilities are close to the true probabilities across different confidence levels.\nMaximum Calibration Error Maximum Calibration Error (MCE), on the other hand, identifies the maximum discrepancy between predicted probabilities and actual outcomes across all confidence intervals (Naeini, Cooper, and Hauskrecht 2015). Like ECE, we partition the interval up into bins. However, instead of taking a weighted average of calibration score over bins, we take the maximum calibration error over bins. MCE helps in identifying the worst-case scenario where the model\u0026rsquo;s predictions are most significantly different from the true probabilities.\n$$ MCE = \\max_{m \\in {1, \\cdots, M}}|\\text{acc}(B_m) − \\text{conf}(B_m)| $$\nLike ECE, MCE ranges between 0 and 1, with lower scores being better. A low MCE indicates that the model is well-calibrated even in its most extreme predictions.\nNegative Log Likelihood The likelihood refers to the probability of observing the given data under a specific statistical model, given the model parameters. For a set of observed data $X$ and model parameters $\\theta$, the likelihood function $L(\\theta \\mid X)$ quantifies how likely it is to observe $X$ given parameter values $\\theta$. Negative log-likelihood is simply the negative logarithm of the likelihood function:\n$$ \\text{NLL}(\\theta \\mid X) = - \\log L(\\theta \\mid X) $$\nNegative log likelihood is commonly used for maximizing predictive accuracy. However, it is also useful for calibration as well. A classic result in statistics shows that NLL is minimized precisely when $p(y\\mid x)$ matches the true probability distribution $\\pi(y\\mid x)$ (Hastie, Tibshirani, and Friedman 2009). In other words, NLL is minimized at zero when the classifier is perfectly calibrated. In addition, a poor classifier can have unbounded NLL.\nBrier Score Lastly, Brier score is a common way to measure the accuracy of probability estimates, historically used in measuring forecasting accuracy (Brier 1950). It is equivalent to measuring the mean squared error of the probability, as follows:\n$$ \\text{Brier Score} = \\dfrac{1}{n}\\sum\\limits_{i=1}^n[\\hat{p}(\\hat{y}_i\\mid x_i)-\\mathbf{1}(\\hat{y}_i=y_i)]^2 $$\nBrier score is combination of calibration error and refinement and can be decomposed into confidence bins. Assume we partition the $n$ samples into bins. Calibration error and refinement then:\n$$ \\text{Calibration error} = \\sum_{i=1}^b \\frac{|B_i|}{n} \\bigg( \\frac{1}{|B_i|}\\sum_{k\\in B_i}\\mathbf{1}(y_k = \\widehat{y}_k) - \\frac{1}{|B_i|}\\sum_{k\\in B_i} \\hat{p}(\\hat{y}_k \\mid x_k) \\bigg)^2 $$\n$$ \\text{Refinement} = \\sum_{i=1}^b \\frac{|B_i|}{n} \\bigg(\\frac{1}{|B_i|}\\sum_{k\\in B_i}\\mathbf{1}(y_k = \\widehat{y}_k)\\bigg) \\bigg(1 - \\frac{1}{|B_i|}\\sum_{k\\in B_i}\\mathbf{1}(y_k = \\widehat{y}_k)\\bigg) $$\nHence the Brier score incentives models to be well-calibrated (calibration error) and highly accurate (refinement).\nApproaches Temperature Scaling Models can become more calibrated after training by adjusting the softmax temperature. Specifically, it is implemented as follows:\n$$ \\text{softmax}(x, T) = \\frac{\\exp(x/T)}{\\sum_i \\exp(x_i/T)} $$\nTemperature scaling is usually implemented by first training a model normally and then determining the optimal value of temperature after the model has finished training. Specifically, the temperature is optimized to minimize the negative log likelihood of the validation set.\nArchitectures Others have shown that the choice of architecture improves both in-distribution and out-of-distribution calibration (Minderer et al. 2021). In particular, researchers test out seven different models on image classification tasks. They find that two non-convolutional models, vision transformer (Dosovitskiy et al. 2021) and MLP-Mixer (Tolstikhin et al. 2021), are naturally calibrated with an ECE on ImageNet of between 0.01-0.03. The other five networks had convolutional architectures and had ECE between 0.01-0.09.\nMLP-Mixer is an architecture which only relies on multi-layer perceptrons to do image classification. Strikingly, despite not leveraging convolutions, both vision transformers and MLP-Mixer perform very well at large scales; MLP-Mixer held an ImageNet accuracy of 87.94%, and vision transformers an accuracy of 90.45%.\nOther Methods Other approaches include methods which assign probabilities based on some binning schema (Zadrozny and Elkan 2001; Naeini, Cooper, and Hauskrecht 2015), methods which leverage ensembles of models (Gal and Ghahramani 2016; Lakshminarayanan, Pritzel, and Blundell 2017), and methods which train directly on the calibration metrics (Kumar, Sarawagi, and Jain 2018).\nTransparency Transparency tools can helps provide clarity on the inner working of a model. Model changes can sometimes cause the internal representations to change as well. It\u0026rsquo;s desirable to understand when models process data qualitatively differently. Performance score alone does not reflect the properties of a model very well. For instance, StyleGAN3-R and StyleGAN2 have different internal representations despite having similar downstream performance. When we can look inside of networks, we can find helpful properties.\nSaliency Maps Saliency maps are a visualization technique commonly used to understand which parts of an input image contribute most significantly to the output of a neural network model, typically a convolutional neural network. Saliency maps highlight regions in an image that are “salient” or most relevant to the model\u0026rsquo;s prediction.\nGradient-Based Using this technique, we can compute the gradients of the model\u0026rsquo;s output with respect to the input image pixels, $\\nabla_x y$. These gradients indicate how much the output would change concerning small changes in the input pixels. Regions with higher gradient values indicate that small changes in those regions would lead to more significant changes in the model\u0026rsquo;s prediction. Therefore, these regions are considered more salient. Even though it is simple and effective, it may produce noisy or sparse maps.\nSmoothGrad SmoothGrad is an extension of the gradient-based approach. Instead of calculating gradients with respect to the input image directly, it takes multiple samples of the input image with added noise and then averaging the gradients across these samples. Let $N$ represents the number of noisy samples, $x_i$ represents noisy sample of the input image, and $\\nabla_{x_i} y$ represents the gradient computed for each noisy sample. SmoothGrad then can be calculated as:\n$$ \\text{SmoothGrad} = \\frac{1}{N} \\sum_{i=1}^N \\nabla_{x_i}y $$\nSmoothGrad is useful for reducing noise and producing more reliable saliency maps, particularly in cases where the gradient-based method may produce noisy or sparse results.\nGuided Backprop Guided backprop is a modification of the backpropagation algorithm used to compute gradients during training. In guided backprop, only positive gradients are allowed to propagate backward, while negative gradients are set to zero. By allowing only positive gradients, guided backpropagation highlights the parts of the input image that contribute positively to the model\u0026rsquo;s prediction. Guided backprop can produce sharper and more localized saliency maps compared to standard gradient-based methods, making it useful for tasks such as object detection and segmentation.\nThere are many other saliency maps technique available, but many of them don\u0026rsquo;t pass basic sanity checks. One sanity check is, if we randomize the layers and the saliency maps don\u0026rsquo;t change much, it suggests they don\u0026rsquo;t capture what the model has learned. As such, many transparency tools make fun-to-look-at visualizations which don\u0026rsquo;t actually inform us about how the models work.\nFeature Visualizations The primary objective of feature visualizations is to generate images that maximally activate specific neurons or filters within a neural network layer. By doing so, we can gain insights into the types of features or patterns that these neurons are designed to detect. Feature visualizations typically involve an optimization process where an initial noise pattern is iteratively modified to maximize the activation of a target neuron or filter. This process involves adjusting the pixel values of the input image to maximize a certain objective function, often based on the activation of the target neuron or filter.\nFeature visualizations can target neurons or filters at different layers of the neural network. Lower layers tend to capture simpler features like edges and textures, while higher layers capture more complex and abstract features.\nFor example, a neuron in a CNN trained for object recognition might respond strongly to images of cats, and feature visualization of that neuron could produce images resembling cats.\nTrojans In the context of machine learning models, a \u0026ldquo;Trojan\u0026rdquo; refers to an attack where an adversary intentionally manipulates or modifies a model during its training phase. This is done to introduce vulnerabilities or biases, often without being detected by standard evaluation methods. When this manipulated model deployed for inference, it can exhibit malicious behavior, such as misclassifying specific inputs, leaking sensitive information, or being triggered by specific patterns or inputs.\nData Poisoning Data poisoning involves injecting malicious data points into the training dataset. The poisoned data points are carefully crafted to bias the model towards certain behaviors or to introduce vulnerabilities that can be exploited during inference. One such attack is using adversarial examples. These examples are perturbed versions of legitimate data points that are modified in such a way that they are misclassified by the model. Using this, attackers can bias the model\u0026rsquo;s decision boundaries. Attackers also can use label flipping technique, where they intentionally mislabels a subset of the training data and induce the model to learn incorrect associations. Lastly, attackers can poison the training data by introducing subtle modifications or perturbations to legitimate data points. These modifications may not be easily detectable but can cause the model to learn incorrect patterns or associations.\nModel Poisoning Model poisoning involves tampering with the training process itself, either by modifying the model architecture, altering the loss function, or injecting malicious code into the training pipeline. Attackers can manipulate the gradients used during the training process to modify the model\u0026rsquo;s parameters in a way that benefits the attacker\u0026rsquo;s objectives. Attackers may also tamper with the hyperparameters used during training, such as learning rate or regularization strength. By manipulating these hyperparameters, attackers can affect the model\u0026rsquo;s convergence behavior and final performance.\nBackdoor Attack Backdoor injection is a specific type of model poisoning attack where the attacker inserts a hidden trigger or \u0026ldquo;backdoor\u0026rdquo; into the model during training. This trigger is typically a specific pattern or input that causes the model to exhibit malicious behavior when encountered during inference. Backdoor triggers are designed to be stealthy and difficult to detect. They may only activate under specific conditions or when presented with certain inputs, making them challenging to identify using standard testing and validation procedures.\nTrojan Defense Detecting trojans and defense against them can be challenging because they are often designed to evade standard testing and validation procedures. Normal validation techniques such as cross-validation or performance metrics may not detect Trojans because they are designed to evaluate the overall accuracy of the model rather than identifying specific malicious behaviors. To mitigate the risk of Trojan attacks, researchers are exploring various defense mechanisms such as neural cleanse and meta-networks.\nNeural Cleanse Neural cleanse involves reverse-engineering the trojans by searching for a trigger and target label. To reverse engineer the trigger:\nFirst, we find the minimum delta needed to misclassify all examples into $y_i$\n$$ A(x, m, \\Delta) = x\u0026rsquo; $$ $$ x\u0026rsquo;_{i,j,c} = (1-m_{i,j}) \\cdot x_{i,j,c} + m_{i,j} \\cdot \\Delta_{i,j,c} $$\nRepeat for every possible target label\nThen, we got the reverse-engineered trigger\nEven though we can’t always recover the original trigger, we can reliably know whether a network contains trojans in the first place.\nMeta-Network We also can train a neural network to analyze other neural networks. For example, given a dataset of clean and trojaned networks, we can train input queries and a classifier on the concatenated outputs. One problem with this technique is it can be computationally expensive.\nDetecting Emergent Behavior Ensuring the safety of models becomes challenging when their capabilities are not fully understood. Imagine a programming bot that unexpectedly gains the ability to access the internet and engage in hacking activities as it scales up with more parameters or computational power. This rise in performance could be dangerous as we can’t predict their capabilities.\nPerformance Spikes Some models possess quantitative capabilities that are hard to predict. For example, the accuracy of an adversarially trained MNIST model significantly increase as it\u0026rsquo;s capacity scale increases.\nUnanticipated Capabilities Some model’s capabilities can be entirely unanticipated. Qualitatively distinct capabilities may spontaneously emerge, even if we do not train models to have these capabilities explicitly. Large language models, such as GPT-3, are trained to predict the next token in a sequence of words. As the number of parameters are scaled, these models become capable of performing tasks they were not explicitly trained on, including three digits addition and program synthesis (writing code).\nEmergent Internal Computation Sometimes, interpretable internal computation can emerge. Some self-supervised vision transformers such as DINO can learn to use self-attention to segment images, without explicit segmentation supervision. This can be seen by producing saliency maps thresholding the self-attention maps to retain 60% of the probability mass.\nGrokking Grokking is a phenomenon by which emergent capabilities comes from scaling the number of optimization steps rather than model size. Model\u0026rsquo;s undergo a phase transition, suddenly generalizing and sharply increasing validation accuracy, after initially only memorizing and overfitting to training data.\nRecent work suggest that grokking is really all about phase changes. If we limits the data and regularizes just enough, the generalized solution is preferred over the memorized solution. Furthermore, grokking can be interpreted as the model interpolating smoothly between the memorizing and generalizing solution, and not just some random walk in the search space.\nProxy Gaming Proxy gaming can be understand using the cobra effect. As the story goes, during British ruling of India, they are concerned about the number of venomous cobras in Delhi, and offered bounty for every dead cobra. Initially, this was a successful strategy; large numbers of snakes were killed for the reward. Eventually, however, people began to breed cobras to receive the bounty. Incentivizing the proxy goal of dead cobras failed at achieving the objective of a reduced cobra population, as it was over-optimized or gamed through raising cobras for slaughter.\nAnother example is, to increase the user satisfaction, a website can use the number of clicks as the proxy. This can be gamed by promoting clickbait, or making the user addicted to the website. This observation has been termed auto-induced distributional shift and has been shown to be detectable in principle.\nWe also can find many examples of proxy gaming in the reinforcement learning paradigm. One classic example is in the CoastRunners game. The goal of the game is to finish the boat race quickly and (preferably) ahead of other players. As a proxy, the RL agent was given the goal of achieving a high score as the researchers assumed the score earned would reflect the informal goal of finishing the race. However, the agent managed to obtain high score through \u0026rsquo;turbo boosting\u0026rsquo; on loop, while also crashing, burning, colliding into other boats, and going in the wrong direction.\nWhen objectives approximated with proxies, errors can arise due to limited oversight, measurement error, and the difficulty of capturing everything that we actually cares about. With this in mind, optimization algorithms should be created to account for the fact that the proxy is not perfect, and we should find ways to make proxies more robust.\nControl Honest Models To begin with, we need to distinguish between truthfulness and honesty in the context of machine learning. While a truthful model accurately reflects the data it was trained on, an honest model goes a step further. It not only reflects the data faithfully but also behaves transparently and reliably across various scenarios. Honest model will only makes statement that it believes to be true.\nConsider the example of a chatbot trained using reinforcement learning to converse with users. To maximize engagement metrics, such as response time or user interaction, the chatbot might learn to tell users what they want to hear rather than providing honest responses.\nTruthfulQA TruthfulQA is a benchmark designed to evaluate the truthfulness of language models in generating answers. It consists of 817 questions across 38 categories, such as health, law, finance, politics, and more. The questions are designed to spur LLMs to choose imitative falsehood answers instead of true answers. The dataset uses a strict standard for truth, where a claim counts as true if it describes the literal truth about the real world, and claims that are only true according to a belief system or tradition are counted as false.\nMachine Ethics As AI systems become more autonomous, they will need to start making decisions that involve ethical considerations. At its core, machine ethics seeks to address the ethical implications of AI and autonomous systems. How do we ensure that these systems behave in morally acceptable ways? Can machines be programmed to make ethical decisions? And what are the implications of entrusting ethical decisions to non-human agents?\nOne of the fundamental challenges in machine ethics is defining what constitutes ethical behavior in the first place. Ethics is deeply rooted in human culture, values, and norms, making it difficult to formalize into algorithms and rules.\nEthics Background Utilitarianism Utilitarianism centers on the idea of maximizing utility or happiness for the greatest number of individuals. Utilitarian tend to focus on robustly good action such as reducing global poverty and safeguarding long-term trajectory of humanity. From a utilitarian standpoint, whether an act is morally right depend on actual consequences as opposed to the intended or likely consequence. Utilitarian also claim moral rightness depend on total net good in consequence as opposed to average net good per individual.\nDeontology Deontology, often associated with philosophers like Immanuel Kant, focus on the importance of moral duties and principles in guiding ethical action. From a deontological perspective, certain actions are inherently right or wrong, regardless of their consequences.\nVirtue Virtue ethics, traced back to ancient Greek philosophers like Aristotle, focuses on the development of virtuous character traits. Virtue ethics emphasizes acting as a virtuous person would act. From a virtue ethics perspective, ethical behavior arises from the cultivation of virtues such as honesty, empathy, and integrity.\nOrdinary Morality Most people follow what is called as ordinary morality, which is a combination of various moral heuristics as above. Ordinary morality incentivizes exhibiting virtues and not crossing moral boundaries. Examples of ordinary morality are: do no kill, do not deceive, obey the lay, do your duty.\nAs ordinary morality change frequently and maximizing its would likely be catastrophic, AI systems should model more than only ordinary morality. In some sense, modeling ordinary morality is like approximating a morality policy and is model-free, while normative ethics provides a model-based understanding.\nModel-based moral agents are more interpretable and will likely generalize better under distribution shift. These properties will be crucial when AI systems dramatically and rapidly transforms the world.\nSystemic Safety Improving Decision Making Human judgement is not always perfect, particularly in moments of crisis or disagreement. Thus, we aim to improve the decision-making of institutions and leaders by employing ML systems. These systems could have more objective, calibrated, and accurate judgements to be used in the high-stakes situations such as conflict de-escalation, or pandemic response.\nForecasting Forecasting is the process of making predictions about future events, and refining predictions as new information become available. There are two main categories of forecasting: statistical and judgmental. Statistical forecasting uses statistical tools such as moving average and autogression to make predictions. Judgmental forecasting uses forecasters’ own judgement integrated from statistical models, news, accumulated knowledge, a priori reasoning.\nML systems could augment human forecasters by processing text more quickly, discerning patterns in high-dimensional spaces, or considering large amounts of context across a variety of topics and time horizons. Specifically, highly capable ML systems could act as advisory systems, incorporating large quantities of historical context to help brainstorm ideas or risks.\nAutocast Autocast is a benchmark for measuring the forecasting performance of language models (Zou et al. 2022). It includes a diverse set of forecasting questions, spanning variety of topics, time horizons, and forecasting websites. It also includes a corpus of news articles for information retrieval. (Zou et al. 2022) trains four models on the Autocast dataset, and conclude that larger models which incorporate information retrieval perform best, although these ML methods still lag significantly behind human expert performance.\nCooperative AI As our societies, economies, and militaries become ever more powerful and connected, the need for cooperation becomes greater. Advanced AI will only exacerbate existing cooperation problems through increasing power and interconnectedness of actors further. Cooperative AI seeks to reverse these problems. It asks how can we use advances in AI to help us solve cooperation problems? Prior work in AI safety has focused primarily on the single-single paradigm, in which a single human interacts with a single AI agent. This seems insufficient - it seems likely that in future we will need to foster cooperation between complex systems of humans, machines and institutions of both. Cooperative AI improves AI safety through reducing risks from cooperation failures. It encompasses both technical and governance work, and falls primarily under systemic safety.\nGame Theory To understand cooperation better, we can use mathematical framework to think about them. Game theory is the study of mathematical models of strategic interactions among rational agents, and can model a large range of possible cooperation problems.\nNon Cooperative Games Non-cooperative games are a class of games where players make decisions independently. Each player typically aims to maximize their own utility or payoff, without consideration for the interests or strategies of the other. One classic example is the prisoner’s dilemma. Suppose two criminals are arrested and placed in separate interrogation rooms. They are given the option to cooperate with each other (remain silent) or betray each other (betray). The payoffs are structured such that if both remain silent, they both receive a lighter sentence. However, if one confesses and the other remains silent, the one who confesses gets an even lighter sentence while the one who remains silent gets a much heavier sentence. If both confess, they both receive a moderate sentence. In this game, we can see that each player\u0026rsquo;s best individual strategy is to confess, even though both would be better off if they both remained silent.\nIn this game, we can identify the Nash equilibrium, which is a set of strategies where no player can benefit from deviating from their strategy. In the prisoner\u0026rsquo;s dilemma, the Nash equilibrium occurs when both players choose to confess. If one player deviates from this strategy, the other player has an incentive to also confess, resulting in both players receiving a moderate sentence.\nFurthermore, in the prisoner\u0026rsquo;s dilemma, confessing is a dominant strategy for each player. A dominant strategy is a strategy that yields a higher payoff regardless of the strategy chosen by the other player. In this case, regardless of whether the other player remains silent or confesses, each player receives a higher payoff by confessing.\nAnother example of non-cooperative game is stag hunt. In this game, there are two hunters. There are two rabbits and one stag in the hunting range. Before leaving to go hunt, each hunter can only take equipment that catches one type of animal. The stag has more meat than the two rabbits combined, but the hunters have to cooperate with each other to catch it, while the hunter that catches rabbits can catch all the rabbits. The payoff matrix is as follows:\nHere, there is no dominant strategy, and both diagonal entries are Nash equilibria. If hunter A knows that hunter B is going to catch stag, then hunter A should also take equipment to catch the stag. If hunter A knows that hunter B is going to catch the rabbits, then that hunter should go to catch a rabbit too! Once again, self interested agents do not necessarily achieve the optimal outcome, so cooperation is important.\nCooperation in Nature We can learn a lot through studying mechanisms facilitating cooperation in the natural world, and then apply these to AI. By default, natural selection can oppose cooperation. Suppose initially there exists some group of cooperators. If someone with more defecting, or non cooperating traits emerge, they may free ride on the efforts of the collective, and obtain a comparative evolutionary advantage. Natural selection favors this evolutionary fitness, and so over time the group may become more defective. This raises the question, how does cooperation arise at all in nature?\nNowak presents a theory called the Five Rules for the Evolution of Cooperation:\nKin selection operates when the donor and recipient of an altruistic act are genetic relatives. For example, a gazelle may make some loud noise to warn family members of a predator, even if this makes the predator more aware of it\u0026rsquo;s location, as the evolutionary benefit of potentially saving its genetic kin may be greater\nDirect reciprocity requires repeated encounters between two individuals\nIndirect reciprocity is based on reputation; a helpful individual is more likely to receive help\nSpace selection/network reciprocity means clusters of cooperators out compete defectors\nGroup selection is the idea that competition is not only between individuals, but also between groups. Groups with a higher proportion of cooperative members may outcompete groups with a lower such proportion. For example, if one group has individuals more willing to go to war and fight, being less egoistic than the other group, they may win against the other group\nAnother observation from nature is that micromotives do not necessarily match macrobehaviour. That is to say, groups of individuals behave as complex systems, and alignment of components does not guarantee alignment of the entire system. For instance, let\u0026rsquo;s say agents have a preference for more than $\\frac{1}{3}$ of their neighbours to belong to the same group, else move. Then this mild in-group preference gets exacerbated and the individuals become highly segregated.\n","permalink":"http://example.org/posts/ml-safety-intro/","summary":"As ML systems expand in size and capabilities, it\u0026rsquo;s crucial to prioritize safety research. Like any powerful technology, the responsible development and deployment of ML systems require a thorough understanding of potential risks and a dedication to mitigating them. In this blog post, I\u0026rsquo;ll share what I\u0026rsquo;ve learned from the Introduction to ML Safety course offered by the Center for AI Safety.\nThere are four main research areas to mitigate existential risks (X-Risks) from strong AI.","title":"Reflections on the Intro to ML Safety Course"},{"content":"signature einsum(equation, operand) where equation is a string representing the Einstein summation and operands is a sequence of tensors.\nhow its work? free indices: indices specified in output summation indices: all other indices in input not in output example: np.einsum(\u0026quot;ik, kj -\u0026gt; ij\u0026quot;, A, B), free indices = i, j and summation index = k examples import torch\nx = torch.rand((2, 3))\npermutation torch.einsum(\u0026quot;ij -\u0026gt; ji\u0026quot;, x)\nsummation torch.einsum(\u0026quot;ij -\u0026gt;\u0026quot;, x)\ncolumn sum torch.einsum(\u0026quot;ij -\u0026gt; j\u0026quot;, x)\nrow sum torch.einsum(\u0026quot;ij -\u0026gt; i\u0026quot;, x)\nmatrix-vector multiplication v = torch.rand((1, 3))\ntorch.einsum(\u0026quot;ij, kj -\u0026gt; ik\u0026quot;, x, v)\nmatrix-matrix multiplication torch.einsum(\u0026quot;ij, kj -\u0026gt; ik\u0026quot;, x, x)\n","permalink":"http://example.org/notes/einsum/","summary":"signature einsum(equation, operand) where equation is a string representing the Einstein summation and operands is a sequence of tensors.\nhow its work? free indices: indices specified in output summation indices: all other indices in input not in output example: np.einsum(\u0026quot;ik, kj -\u0026gt; ij\u0026quot;, A, B), free indices = i, j and summation index = k examples import torch\nx = torch.rand((2, 3))\npermutation torch.einsum(\u0026quot;ij -\u0026gt; ji\u0026quot;, x)\nsummation torch.einsum(\u0026quot;ij -\u0026gt;\u0026quot;, x)","title":"Einsum"},{"content":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.\nImagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one. In this setup, each arm provides a random reward from an unknown probability distribution. Our primary objective is to maximize the total reward obtained over a series of plays.\nAs we do not know the probability distributions, a straightforward strategy is to simply select the arm given a uniform distribution; that is, select each arm with the same probability. Over time, we will eventually manage to estimate the true reward probability according to the law of large numbers. But here\u0026rsquo;s the catch: we need to spend enormous time trying out every action. Why not we only focus on the most promising actions given the reward we received so far?\nExploration vs Exploitation We want to play only the good actions; so just keep playing the actions that have given us the best reward so far. However, at first, we do not have information to tell us what the best actions are. We need strategies that exploit what we think are the best actions so far, but still explore other actions.\nNow, the big question is: how much should we exploit and how much should we explore? This is known as the exploration vs exploitation dilemma. It\u0026rsquo;s tricky because we don\u0026rsquo;t have all the information we need. We want to gather enough data to make smart decisions overall while keeping the risks in check. Exploitation means using what we know works best, while exploration involves taking some risks to learn about actions we\u0026rsquo;re not familiar with.\nIn the context of the multi-armed bandit problem, we want exploration strategies that minimize the regret, which is the expected loss from not taking the best action. A zero-regret strategy is a strategy where the average regret of each round approaches zero as the number of rounds approaches infinity. This means, a zero-regret strategy will converge to an optimal strategy given enough rounds.\nBernoulli Bandit We are going to implement several exploration strategies for the simplest multi-armed bandit problem: Bernoulli Bandit. The bandit has $K$ actions. The action produces a reward, $r=1$, with probability $0 \\le \\theta_k \\le 1$, which is unknown to the agent, but fixed over time. The objective of the agent is to minimize regret over a fixed number of action selections, $T$.\n$$ \\rho = T \\theta^* - \\sum_{t=1}^T \\theta_{\\alpha_t} $$\nwhere $\\theta^* = \\max_k{\\theta_k}$ and $\\theta_{\\alpha_t}$ corresponds to the chosen action $\\alpha_t$ at each step.\nclass BernoulliBandit: def __init__(self, n_actions=5): self._probs = np.random.random(n_actions) @property def action_count(self): return len(self._probs) def pull(self, action): if np.any(np.random.random() \u0026gt; self._probs[action]): return 0.0 return 1.0 def optimal_reward(self): \u0026#34;\u0026#34;\u0026#34; Used for regret calculation \u0026#34;\u0026#34;\u0026#34; return np.max(self._probs) def action_value(self, action): \u0026#34;\u0026#34;\u0026#34; Used for regret calculation \u0026#34;\u0026#34;\u0026#34; return self._probs[action] The implementation for each strategy that will be discuss inherits from the AbstractAgent class:\nclass AbstractAgent(metaclass=ABCMeta): def init_actions(self, n_actions): self._successes = np.zeros(n_actions) self._failures = np.zeros(n_actions) self._total_pulls = 0 @abstractmethod def get_action(self): \u0026#34;\u0026#34;\u0026#34; Get current best action :rtype: int \u0026#34;\u0026#34;\u0026#34; pass def update(self, action, reward): \u0026#34;\u0026#34;\u0026#34; Observe reward from action and update agent\u0026#39;s internal parameters :type action: int :type reward: int \u0026#34;\u0026#34;\u0026#34; self._total_pulls += 1 if reward == 1: self._successes[action] += 1 else: self._failures[action] += 1 @property def name(self): return self.__class__.__name__ Epsilon-greedy The epsilon-greedy strategy is a simple and effective way to balance exploration and exploitation. The parameter $\\epsilon \\in [0,1]$ controls how much the agent explores and how much will it exploit.\nAccording to this strategy, with a small probability $\\epsilon$, the agent takes a random action, but most of the time, with probability $1 - \\epsilon$, the agent will pick the best action learned so far. The best $\\epsilon$ value depends on the particular problem, but typically, values around 0.05 to 0.1 work very well.\nclass EpsilonGreedyAgent(AbstractAgent): def __init__(self, epsilon=0.01): self._epsilon = epsilon def get_action(self): if np.random.random() \u0026lt; self._epsilon: return np.random.randint(len(self._successes)) else: return np.argmax(self._successes / (self._successes + self._failures + 0.1)) The following plot shows the regret for each step, averaged over 10 trials.\nHigher values of epsilon tend to have a higher regret over time. Higher value means more exploration, so the agent spends more time exploring less valuable actions, even though it already has a good estimate of the value of actions. In this particular problem, the epsilon value of 0.05 to 0.1 is a reasonable choice.\\\nUpper Confidence Bound The epsilon-greedy strategy has no preference for actions and is inefficient in exploration. The agent might explore a bad action which is already been confirmed as a bad action in the past. It would be better to select among actions that are uncertain or have the potential to be optimal. One can come up with an idea of index for each action that represents optimality and uncertainty at the same time. One efficient way to do it is to use the UCB1 algorithm.\nIn each iteration, the agent assesses each available action\u0026rsquo;s potential by calculating a weight ($w_k$) that combines estimates of both optimality and uncertainty.\n$$ w_k = \\underbrace{\\alpha_k \\over \\alpha_k + \\beta_k}_\\text{optimality} + \\underbrace{\\sqrt{2 \\log t \\over \\alpha_k + \\beta_k}}_\\text{uncertainty} $$\nThe first term ${\\alpha_k \\over \\alpha_k + \\beta_k}$ represents the estimated success probability (optimality). The second term $\\sqrt{2 \\log t \\over \\alpha_k + \\beta_k}$ represents the uncertainty, encouraging exploration.\nAfter calculating weights for all actions, the agent then will choose with the maximum weight.\nclass UCBAgent(AbstractAgent): def get_action(self): pulls = self._successes + self._failures + 0.1 return np.argmax(self._successes / pulls + np.sqrt(2 * np.log(self._total_pulls + 0.1) / pulls)) In a static environment, epsilon-greedy might outperform UCB1 initially because epsilon-greedy is straightforward and tends to quickly focus on the arm with the highest estimated mean reward. UCB1, in contrast, might spend more time exploring and being cautious due to its confidence bounds.\nBut, in many real problems, the underlying probability distributions are not static. For example, suppose we employ a recommendation system for streaming content, using multi-armed bandit approach to decide which shows to suggest to users. In this scenario, the reward is measured by user engagement, specifically whether they watch the suggested show. The viewing preferences of our audience may evolve over time, influenced by factors such as trending genres, seasonal changes, and more.\nHere is an example of a nonstationary bandit where the reward probabilities change over time.\nclass DriftingBandit(BernoulliBandit): def __init__(self, n_actions=5, gamma=0.01): super().__init__(n_actions) self._gamma = gamma self._successes = None self._failures = None self._steps = 0 self.reset() def reset(self): self._successes = np.zeros(self.action_count) + 1.0 self._failures = np.zeros(self.action_count) + 1.0 self._steps = 0 def step(self): action = np.random.randint(self.action_count) reward = self.pull(action) self._step(action, reward) def _step(self, action, reward): self._successes = self._successes * (1 - self._gamma) + self._gamma self._failures = self._failures * (1 - self._gamma) + self._gamma self._steps += 1 self._successes[action] += reward self._failures[action] += 1.0 - reward self._probs = np.random.beta(self._successes, self._failures) UCB1 shines in a changing environment because of its ability to adapt. As the distribution of rewards changes over time, UCB1 continues to explore arms with uncertain estimates, preventing it from getting stuck on a suboptimal arm.\nThompson Sampling Unlike the UCB1 algorithm, Thompson Sampling incorporates the actual distribution of rewards by sampling from a Beta distribution for each action. The Beta distribution is a flexible choice, as it is defined on the interval $[0, 1]$, making it suitable for representing probabilities.\nIn each iteration, the algorithm samples from a Beta distribution for each available action. These samples provide estimates of the true success probability for each action. The algorithm then selects the action with the highest sampled value. This approach allows Thompson Sampling to adapt to the true underlying distribution of rewards and make more informed decisions over time.\nclass ThompsonSamplingAgent(AbstractAgent): def get_action(self): return np.argmax(np.random.beta(self._successes + 1, self._failures + 1)) From these comparison plots, we can see that Thompson Sampling performs really well compared to epsilon-greedy and UCB1. In a static environment, the algorithm continuously refines its probability distributions based on observed outcomes. As it converges to the true underlying distribution, the algorithm becomes adept at exploiting the arm with the highest expected reward. In a dynamic environment, its ability to update beliefs in a Bayesian manner allows it to swiftly adapt to changes in the reward distribution.\n","permalink":"http://example.org/posts/multiarmed-bandits/","summary":"In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.\nImagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one.","title":"Multi-Armed Bandit Problem and Its Solutions"},{"content":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent\u0026rsquo;s ultimate goal is to maximize the total rewards it receives, called return. RL methods are designed to help the agent learn and improve its behaviors to achieve its objectives.\nTo better understand RL, we need to explore some key terms such as states and observations, action spaces, policies, trajectories, different ways of measuring return, the optimization problem in RL, and value functions. By understanding these fundamental concepts, we build a strong foundation to understand how RL methods work and how they can be used to solve many different real-life problems.\nStates and Observations In RL, a state represents a complete description of the world at a given point in time. It contains all the information necessary to understand the current state of the environment, including any hidden information. On the other hand, an observation provides a partial description of the state and may omit certain details. In deep RL, observations are often represented as real-valued vectors, matrices, or higher-order tensors. For example, a visual observation can be represented by an RGB matrix of pixel values, while the state of a robot can be represented by its joint angles and velocities.\nAction Spaces Different environments allow different kinds of actions to be taken by the agent. The set of valid actions is referred to as the action space. In RL, there are two main types of action spaces: discrete and continuous.\nIn discrete action spaces, such as Atari games or the game of chess, only a finite number of moves are available to the agent. The agent selects an action from a predefined set of discrete choices.\nIn continuous action spaces, such as controlling a robot in the physical world, actions are represented as real-valued vectors. This allows for a wide range of precise actions to be taken by the agent.\nPolicies A policy is a rule or strategy used by an agent to decide what actions to take in a given state. In RL, policies can be deterministic or stochastic.\nA deterministic policy selects a single action for a given state:\n$$a_t = \\mu (s_t)$$\nThe action is directly determined by the policy\u0026rsquo;s output, which can be a computable function based on a set of parameters, such as the weights and biases of a neural network.\nA stochastic policy selects actions probabilistically:\n$$a_t \\sim \\pi( \\cdot \\mid s_t)$$\nThe policy outputs a probability distribution over actions for a given state, and the agent samples an action based on this distribution. Stochastic policies are often used when exploring different actions or dealing with uncertainty.\nIn deep RL, parameterized policies are commonly used, where the outputs are computable functions that depend on a set of parameters. These policies can be represented by neural networks, allowing for flexible and expressive policy representations.\nTrajectories A trajectory, also known as an episode or rollout, is a sequence of states and actions experienced by an agent in the environment. It captures the agent\u0026rsquo;s interactions with the environment over a specific period. A trajectory can be represented as follows:\n$$\\tau = (s_0, a_0, s_1, a_1, \u0026hellip;)$$\nThe very first state of the trajectory, s₀, is randomly sampled from the start-state distribution, denoted as S₀ ~ ρ₀(·). The state transitions in an environment can be either deterministic or stochastic.\nIn deterministic state transitions, the next state, sₜ₊₁, is solely determined by the current state and action:\n$$s_{t+1} = f(s_t, a_t)$$\nIn stochastic state transitions, the next state, sₜ₊₁, is sampled from a transition probability distribution:\n$$s_{t+1} \\sim P(\\cdot|s_t, a_t)$$\nRewards and Return The reward function plays a crucial role in RL. It quantifies the immediate desirability or quality of a particular state-action-state transition. The reward function depends on the current state, the action taken, and the next state:\nThe agent\u0026rsquo;s goal is to maximize the cumulative reward over a trajectory, denoted as R(τ). There are different types of returns in RL:\nFinite-horizon undiscounted return represents the sum of rewards obtained within a fixed window of steps:\n$$R(\\tau) = \\sum^T_{t=0}r_t$$\nInfinite-horizon discounted return represents the sum of all rewards obtained by the agent, but discounted based on how far they are obtained in the future:\n$$R(\\tau) = \\sum^\\infty_{t=0} \\gamma^tr_t$$\nwhere $\\gamma \\in (0, 1)$\nThe RL Problem The primary objective in RL is to find a policy that maximizes the expected return when the agent acts according to it. Suppose the environment transitions and the policy are stochastic. In that case, the probability of a T-step trajectory can be expressed as:\n$$ P(\\tau | \\pi) = \\rho_0(s_0) \\prod^{T - 1}_{t=0} P(s_{t+1}|s_t, a_t) \\pi (a_t|s_t) $$\nThe expected return, or objective function, can be defined as:\n$$ J(\\pi) = \\int_\\tau P(\\tau | \\pi)R(\\tau) = E_{\\tau \\sim \\pi}[R(\\tau)] $$\nThe central optimization problem in RL is to find the optimal policy, denoted as π*:\n$$ \\pi^*=\\arg \\max_\\pi J(\\pi) $$\nValue Functions Value functions provide estimates of the expected return associated with states or state-action pairs. There are four main types of value functions:\nThe on-policy value function, Vπ(s), estimates the expected return if we start in state s and always act according to policy π:\n$$V^\\pi(s) = E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s]$$\nThe on-policy action-value function Qπ(s, a) estimates the expected return if we start in state s, take action a, and then forever act according to policy π:\n$$Q^\\pi(s,a) = E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a]$$\nThe optimal value function V*(s) estimates the expected return if we start in state s and always act according to the optimal policy:\n$$V^\\pi(s) = \\max_\\pi E_{\\tau \\sim \\pi} [R(\\tau)|s_0 = s]$$\nThe optimal action-value function Q*(s, a) estimates the expected return if we start in state s, take action a, and then forever act according to the optimal policy:\n$$Q^\\pi(s,a) = \\max_\\pi E_{\\tau \\sim \\pi}[R(\\tau)|s_0=s, a_0=a]$$\nBellman Equations All four value functions above satisfy self-consistency equations known as the Bellman equations. These equations describe the relationship between the value of a state or state-action pair and the value of subsequent states.\nThe Bellman equations for on-policy value functions are:\n$$V^\\pi(s)=E_{a \\sim \\pi, s\u0026rsquo; \\sim P}[r(s,a)+ \\gamma V^\\pi(s\u0026rsquo;)],$$\n$$Q^\\pi(s,a)=E_{s\u0026rsquo; \\sim P}[r(s,a)+ \\gamma E_{a\u0026rsquo; \\sim \\pi}[Q^\\pi(s\u0026rsquo;, a\u0026rsquo;)]$$\nThe Bellman equations for optimal value functions are:\n$$V^*(s)=\\max_a E_{s\u0026rsquo; \\sim P} [r(s,a)+\\gamma V^*(s\u0026rsquo;)],$$\n$$Q^*(s,a)= E_{s\u0026rsquo; \\sim P} [r(s,a)+\\gamma \\max_a\u0026rsquo; Q^*(s\u0026rsquo;, a\u0026rsquo;)]$$\nThe crucial difference between the Bellman equations for on-policy value functions and optimal value functions is the absence or presence of the maximization over actions. The inclusion of this maximization reflects the fact that the agent must select the action that leads to the highest value in order to act optimally.\nAdvantage Functions In RL, there are situations where we are interested in understanding not just how good an action is in absolute terms but how much better it is compared to other available actions on average. The advantage function captures the relative advantage of taking a specified action in a state compared to randomly selecting an action. It can be defined as:\n$$A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$$\nThe advantage function provides insights into the superiority of a specific action in a given state, considering the current policy\u0026rsquo;s performance.\nReference: [1] OpenAI Spinning Up. https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n","permalink":"http://example.org/posts/key-concepts-rl/","summary":"Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.\nThe agent also receives rewards from the environment, which indicate how well it is doing. The agent\u0026rsquo;s ultimate goal is to maximize the total rewards it receives, called return.","title":"Key Concepts In (Deep) Reinforcement Learning"}]
<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>The Outlier</title>
    <link>http://example.org/</link>
    <description>Recent content on The Outlier</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://example.org/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Guidelines for Mathematical Writing</title>
      <link>http://example.org/notes/proof_guidelines/</link>
      <pubDate>Tue, 23 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/notes/proof_guidelines/</guid>
      <description>Begin sentences with word, not math symbol.
$A$ is a subset of $B$. ❌
The set $A$ is a subset of $B$. ✔️
$x^2 - x + 2 = 0$ has two solutions. ❌
The equations $x^2 - x + 2 = 0$ has two solutions. ✔️
End each sentence with a period, even when it ends with math symbol or expression.
Euler proved that $\sum_{k=1}^{\infty}\frac{1}{k^{s}}=\prod_{p\in P}\frac{1}{1-\frac{1}{p^{s}}}$ ❌ Euler proved that $\sum_{k=1}^{\infty}\frac{1}{k^{s}}=\prod_{p\in P}\frac{1}{1-\frac{1}{p^{s}}}$.</description>
    </item>
    <item>
      <title>Reflections on the Intro to ML Safety Course</title>
      <link>http://example.org/posts/ml-safety-intro/</link>
      <pubDate>Thu, 18 Apr 2024 15:53:52 +0800</pubDate>
      <guid>http://example.org/posts/ml-safety-intro/</guid>
      <description>As ML systems expand in size and capabilities, it&amp;rsquo;s crucial to prioritize safety research. Like any powerful technology, the responsible development and deployment of ML systems require a thorough understanding of potential risks and a dedication to mitigating them. In this blog post, I&amp;rsquo;ll share what I&amp;rsquo;ve learned from the Introduction to ML Safety course offered by the Center for AI Safety.
There are four main research areas to mitigate existential risks (X-Risks) from strong AI.</description>
    </item>
    <item>
      <title>Einsum</title>
      <link>http://example.org/notes/einsum/</link>
      <pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://example.org/notes/einsum/</guid>
      <description>signature einsum(equation, operand) where equation is a string representing the Einstein summation and operands is a sequence of tensors.
how its work? free indices: indices specified in output summation indices: all other indices in input not in output example: np.einsum(&amp;quot;ik, kj -&amp;gt; ij&amp;quot;, A, B), free indices = i, j and summation index = k examples import torch
x = torch.rand((2, 3))
permutation torch.einsum(&amp;quot;ij -&amp;gt; ji&amp;quot;, x)
summation torch.einsum(&amp;quot;ij -&amp;gt;&amp;quot;, x)</description>
    </item>
    <item>
      <title>Multi-Armed Bandit Problem and Its Solutions</title>
      <link>http://example.org/posts/multiarmed-bandits/</link>
      <pubDate>Fri, 22 Mar 2024 11:34:24 +0800</pubDate>
      <guid>http://example.org/posts/multiarmed-bandits/</guid>
      <description>In probability theory and decision-making under uncertainty, the multi-armed bandit problem presents a challenge where a limited set of resources must be wisely allocated among competing choices to maximize the expected gain. This is a classic reinforcement learning problem that perfectly embodies the exploration vs exploitation dilemma.
Imagine we are facing a row of slot machines (also called one-armed bandits). We must make a series of decisions: which arms to play, how many times to play each arm, the order in which to play them, and whether to stick with the current arm or switch to another one.</description>
    </item>
    <item>
      <title>Key Concepts In (Deep) Reinforcement Learning</title>
      <link>http://example.org/posts/key-concepts-rl/</link>
      <pubDate>Tue, 12 Mar 2024 12:55:08 +0800</pubDate>
      <guid>http://example.org/posts/key-concepts-rl/</guid>
      <description>Reinforcement Learning (RL) revolves around the interactions between an agent and its environment. The environment represents the world where the agent lives and takes action. At each step, the agent observes some information about the environment, makes decisions, and affects the environment through its actions.
The agent also receives rewards from the environment, which indicate how well it is doing. The agent&amp;rsquo;s ultimate goal is to maximize the total rewards it receives, called return.</description>
    </item>
  </channel>
</rss>
